{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff0bf73",
   "metadata": {},
   "source": [
    "# BNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed3ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_models import Pbnn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from doepy import build\n",
    "import pickle\n",
    "from scipy.stats import norm, uniform, lognorm\n",
    "from scipy.stats import qmc    #for sobol seq. (LHS is also available in this QuasiMC library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bf0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LStates.g11D_electric import example_electric\n",
    "\n",
    "function = example_electric\n",
    "\n",
    "def convert_lognormal(mean_ln, std_ln):\n",
    "    gaussian_param = np.zeros(2)\n",
    "\n",
    "    SigmaLogNormal = np.sqrt( np.log(1+(std_ln/mean_ln)**2))\n",
    "    MeanLogNormal = np.log( mean_ln ) - SigmaLogNormal**2/2\n",
    "\n",
    "    gaussian_param[0] = MeanLogNormal\n",
    "    gaussian_param[1] = SigmaLogNormal\n",
    "\n",
    "    return gaussian_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f4825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = L (m) mean=4.2    cov=0.10   lognormal/R>0\n",
    "x1_mean = 4.20\n",
    "x1_std = x1_mean * 0.10\n",
    "normal_std_1 = np.sqrt(np.log(1 + (x1_std/x1_mean)**2))\n",
    "normal_mean_1 = np.log(x1_mean) - normal_std_1**2 / 2\n",
    "\n",
    "# X2 = h (m) mean=0.02    cov=0.10 lognormal /R>0\n",
    "x2_mean = 0.02\n",
    "x2_std = x2_mean * 0.1\n",
    "normal_std_2 = np.sqrt(np.log(1 + (x2_std/x2_mean)**2))\n",
    "normal_mean_2 = np.log(x2_mean) - normal_std_2**2 / 2\n",
    "\n",
    "# X3 = d (m) mean=0.001  cov=0.05 lognormal /R>0\n",
    "x3_mean = 0.001\n",
    "x3_std = x3_mean * 0.05\n",
    "normal_std_3 = np.sqrt(np.log(1 + (x3_std/x3_mean)**2))\n",
    "normal_mean_3 = np.log(x3_mean) - normal_std_3**2 / 2\n",
    "\n",
    "# X4 = ZL () mean=1000    cov=0.20 lognormal /R>0\n",
    "x4_mean = 1000\n",
    "x4_std = x4_mean * 0.2\n",
    "normal_std_4 = np.sqrt(np.log(1 + (x4_std/x4_mean)**2))\n",
    "normal_mean_4 = np.log(x4_mean) - normal_std_4**2 / 2\n",
    "\n",
    "# X5 = Z0 () mean=50   cov=0.05 lognormal /R>0\n",
    "x5_mean = 50\n",
    "x5_std = x5_mean * 0.05\n",
    "normal_std_5 = np.sqrt(np.log(1 + (x5_std/x5_mean)**2))\n",
    "normal_mean_5 = np.log(x5_mean) - normal_std_5**2 / 2\n",
    "\n",
    "# X6 = ae (V/m) mean=1  cov=0.20 lognormal /R>0\n",
    "x6_mean = 1\n",
    "x6_std = x6_mean * 0.2\n",
    "normal_std_6 = np.sqrt(np.log(1 + (x6_std/x6_mean)**2))\n",
    "normal_mean_6 = np.log(x6_mean) - normal_std_6**2 / 2\n",
    "\n",
    "# X7 = theta_e (rad) mean=pi/4    cov=0.577 uniform / [0,pi/2]\n",
    "x7_min = 0\n",
    "x7_max = np.pi / 2\n",
    "\n",
    "# X8 = theta_p (rad) mean=pi/4    cov=0.577 uniform / [0,pi/2]\n",
    "x8_min = 0\n",
    "x8_max = np.pi / 2\n",
    "\n",
    "# X9 = phi_p (rad) mean=pi   cov=0.577 uniform / [0,pi*2]\n",
    "x9_min = 0\n",
    "x9_max = np.pi*2\n",
    "\n",
    "# X10 = f (MHz) mean=30    cov=0.096 uniform / [25 ,35]\n",
    "x10_min = 25e6\n",
    "x10_max = 35e6\n",
    "\n",
    "# X11 = alpha (-) mean=0.0010  cov=0.289 uniform / [0.0005 , 0.0015]\n",
    "x11_min = 0.0005\n",
    "x11_max =  0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a460188d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.000209, 3.528450573269899)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 11\n",
    "n_mcs = int(1e6)\n",
    "X = np.zeros((n_mcs,dim))\n",
    "L = X[:,0] = np.random.lognormal(mean=normal_mean_1, sigma=normal_std_1, size=n_mcs)   #L (m)\n",
    "h = X[:,1] = np.random.lognormal(mean=normal_mean_2, sigma=normal_std_2, size=n_mcs)   #h (m)\n",
    "d = X[:,2] = np.random.lognormal(mean=normal_mean_3, sigma=normal_std_3, size=n_mcs)   #d (m)\n",
    "ZL = X[:,3] = np.random.lognormal(mean=normal_mean_4, sigma=normal_std_4, size=n_mcs)   #ZL ()\n",
    "Z0 = X[:,4] = np.random.lognormal(mean=normal_mean_5, sigma=normal_std_5, size=n_mcs)   #Z0 ()\n",
    "ae = X[:,5] = np.random.lognormal(mean=normal_mean_6, sigma=normal_std_6, size=n_mcs)   #ae (V/m)\n",
    "theta_e = X[:,6] = np.random.uniform(low=x7_min, high=x7_max, size=n_mcs)        #theta_e (rad)\n",
    "theta_p = X[:,7] = np.random.uniform(low=x8_min, high=x8_max, size=n_mcs)        #theta_p (rad)\n",
    "phi_p =  X[:,8] = np.random.uniform(low=x9_min, high=x9_max, size=n_mcs)        #phi_p (rad)\n",
    "f = X[:,9] = np.random.uniform(low=x10_min, high=x10_max, size=n_mcs)      #f (MHz)\n",
    "alpha = X[:,10] = np.random.uniform(low=x11_min, high=x11_max, size=n_mcs)     #alpha (-)\n",
    "\n",
    "y_test = function(X)\n",
    "y_max = np.max(y_test)\n",
    "Pf_ref = np.sum( y_test < 0 ) / n_mcs\n",
    "B_ref= - norm.ppf( Pf_ref )\n",
    "Pf_ref, B_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "927a5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = {'x1': [x1_mean, x1_std, 'lognormal'],\n",
    " 'x2': [x2_mean, x2_std, 'lognormal'],\n",
    " 'x3': [x3_mean, x3_std, 'lognormal'],\n",
    " 'x4': [x4_mean, x4_std, 'lognormal'],\n",
    " 'x5': [x5_mean, x5_std, 'lognormal'],\n",
    " 'x6': [x6_mean, x6_std, 'lognormal'],\n",
    " 'x7': [x7_min, x7_max, 'uniform'],\n",
    " 'x8': [x8_min, x8_max, 'uniform'],\n",
    " 'x9': [x9_min, x9_max, 'uniform'],\n",
    " 'x10': [x10_min, x10_max, 'uniform'],\n",
    " 'x11': [x11_min, x11_max, 'uniform']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4ea2bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:  1 ########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathan/environments/deeplearning/lib/python3.9/site-packages/tensorflow_probability/python/layers/util.py:98: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  loc = add_variable_fn(\n",
      "/Users/jonathan/environments/deeplearning/lib/python3.9/site-packages/tensorflow_probability/python/layers/util.py:108: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use the `layer.add_weight()` method instead.\n",
      "  untransformed_scale = add_variable_fn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BNN With 50 samples\n"
     ]
    }
   ],
   "source": [
    "number_experiments = 1\n",
    "passive_samples = 50\n",
    "active_samples = 50\n",
    "output = 1\n",
    "layers, archit = 2 , [50, 50]  #change archit size if layers are increased\n",
    "bnn_simulations = 100\n",
    "training_epochs = 5000\n",
    "n_mcs = int(1e6)\n",
    "learning_points = 5\n",
    "\n",
    "for experiments in range(number_experiments):\n",
    "    print('Experiment: ', experiments+1 , '########################################################' )\n",
    "    \n",
    "    ActiveTrain_1 = {}\n",
    "\n",
    "    #PASSIVE TRAINING---------------------------------------------------\n",
    "    X = np.zeros((passive_samples, dim))\n",
    "    X_norm = np.zeros((passive_samples, dim))\n",
    "   \n",
    "    exp_norm = {}\n",
    "    for var_name in range(dim):\n",
    "        exp_norm['x'+ str(var_name+1)] = [0.000001, 0.999999]    #initial design domain for each variable (normal, uniform)\n",
    "\n",
    "    #Latin hypercube sampling\n",
    "    Xdoe = build.space_filling_lhs(exp_norm , num_samples = passive_samples)\n",
    "\n",
    "    for margin in range (0, dim):\n",
    "        var = 'x' + str (margin + 1)\n",
    "        X_norm[:, margin] = Xdoe[var]\n",
    "\n",
    "        if exp[var][2] == 'normal':\n",
    "            loc_ = exp[var][0]\n",
    "            scale_ = exp[var][1]\n",
    "            X[:, margin] = norm.ppf(Xdoe[var], loc=loc_, scale=scale_)\n",
    "\n",
    "        elif exp[var][2] == 'uniform':\n",
    "            loc_ = exp[var][0]\n",
    "            scale_ = exp[var][1]\n",
    "            X[:, margin] = uniform.ppf(Xdoe[var], loc=loc_, scale=scale_-loc_)\n",
    "\n",
    "        elif exp[var][2] == 'lognormal':\n",
    "            xlog_mean = exp[var][0]\n",
    "            xlog_std = exp[var][1]\n",
    "            gaussian_param = convert_lognormal(xlog_mean, xlog_std)\n",
    "            X[:, margin] = lognorm.ppf(Xdoe[var], s=gaussian_param[1], scale=xlog_mean) \n",
    "\n",
    "    Y = function(X)\n",
    "    Y_norm = Y / y_max\n",
    "\n",
    "    #ACTIVE TRAIN LOOP ---------------------------------------------------\n",
    "    for active in range(passive_samples, active_samples+passive_samples+1):\n",
    "        #-------------------------------------------creating bnn\n",
    "        #setting up the network architecture -----------------------------------\n",
    "        config = {\"n_infeatures\": dim,\n",
    "                \"n_outfeatures\": output,\n",
    "                \"n_samples\": len(X_norm),\n",
    "                \"learn_all_params\": False,  #to learn mean and sigma\n",
    "                \"fixed_param\": 0.001} \n",
    "        \n",
    "        ModelName = 'BNN_' + str(len(X_norm))\n",
    "        mybnn = Pbnn(config)\n",
    "        \n",
    "        mybnn.build_bnn(layers, archit) #----------------------------------------------------------MODEL ARCHITECTURE\n",
    "        #-------------------------------------------training bnn\n",
    "        \n",
    "        # batch_size = len(X)\n",
    "        batch_size = 8\n",
    "        # batch_size = [np.floor_divide(len(X), 10)][0] + 1  # splitting DoE\n",
    "\n",
    "        train_env = {\"batch_size\": batch_size,\n",
    "                    \"learning_rate\": 0.001,\n",
    "                    \"epochs\": training_epochs,\n",
    "                    \"callback_patience\": 500,\n",
    "                    \"verbose\": 0,\n",
    "                    \"valid_split\":0.0}\n",
    "        \n",
    "        print('Training BNN With', len(X_norm), 'samples' )\n",
    "        history = mybnn.train_bnn(X_norm, Y_norm, train_env)\n",
    "\n",
    "        #-------------------------------------------MC population\n",
    "        Xtest = np.zeros((int(n_mcs), dim))\n",
    "        MCinputs_norm= np.random.uniform(0.000001, 0.999999, size=(int(n_mcs), dim))\n",
    "\n",
    "        for margin in range (0, dim):\n",
    "            var = 'x' + str (margin + 1)\n",
    "\n",
    "            if exp[var][2] == 'normal':\n",
    "                loc_ = exp[var][0]\n",
    "                scale_ = exp[var][1]\n",
    "                Xtest[:, margin] = norm.ppf(MCinputs_norm[:, margin], loc=loc_, scale=scale_)\n",
    "\n",
    "            elif exp[var][2] == 'uniform':\n",
    "                loc_ = exp[var][0]\n",
    "                scale_ = exp[var][1]\n",
    "                Xtest[:, margin] = uniform.ppf(MCinputs_norm[:, margin], loc=loc_, scale=scale_-loc_)\n",
    "\n",
    "            elif exp[var][2] == 'lognormal':\n",
    "                xlog_mean = exp[var][0]\n",
    "                xlog_std = exp[var][1]\n",
    "                gaussian_param = convert_lognormal(xlog_mean, xlog_std)\n",
    "                Xtest[:, margin] = lognorm.ppf(MCinputs_norm[:, margin], s=gaussian_param[1], scale=xlog_mean) \n",
    "\n",
    "        #-------------------------------------------model predictions over MC population\n",
    "        print('BNN predictions with MC population...')\n",
    "        Mean_muY_MC, Stdv_muY_MC, Mean_sigmaY_MC, Stdv_sigmaY_MC = mybnn.modeluq_bnn(MCinputs_norm, nsim = bnn_simulations)\n",
    "        y_mcs = function(Xtest)\n",
    "\n",
    "        PF = np.sum(Mean_muY_MC*y_max < 0) / n_mcs\n",
    "        B = - norm.ppf( PF )\n",
    "        PF_ref = np.sum(y_mcs < 0) / n_mcs\n",
    "\n",
    "        print('PF_ref =', PF_ref, 'PF =', PF, 'and B =',\"%.5f\" % round(B, 3) ,'-------------------------------')\n",
    "        print(' ')\n",
    "        #-------------------------------------------Selecting new training point\n",
    "        U_f = np.abs(Mean_muY_MC) / Stdv_muY_MC\n",
    "        U_min_args = np.argsort(U_f.reshape(-1))     #ordering arguments from min to max\n",
    "        X_new = Xtest[U_min_args[:learning_points]]  #choosing a given number of MC samples from the minimum U values\n",
    "        X_new_norm = MCinputs_norm[U_min_args[:learning_points]]  #choosing a given number of MC samples from the minimum U values\n",
    "        X_norm = np.concatenate((X_norm, X_new_norm), axis=0)\n",
    "\n",
    "        Y_new = function(X_new)\n",
    "        Y_norm = np.concatenate((Y_norm, Y_new/y_max), axis=0)\n",
    "        Y = np.concatenate((Y, Y_new), axis=0)\n",
    "\n",
    "        #-------------------------------------------Saving results\n",
    "        ActiveTrain_1[ModelName] = mybnn.weights, PF, B\n",
    "        filename1 = 'Batch_'+ str(experiments+1)+'.sav'\n",
    "    \n",
    "        pickle.dump(ActiveTrain_1, open(filename1, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd4c11d",
   "metadata": {},
   "source": [
    "to sample with sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = qmc.Sobol(d=2, scramble=True)    #d=dimensionality\n",
    "sample = sampler.random_base2(m=5)   #change m=exponent to increase the sample size\n",
    "\n",
    "l_bounds = [-2.0, -2.0]  #design domain for each variable in the physical space\n",
    "u_bounds = [2.0, 2.0]\n",
    "X = qmc.scale(sample, l_bounds, u_bounds)\n",
    "\n",
    "Y = function(X)\n",
    "passive_samples = len(X)\n",
    "print(passive_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ded18",
   "metadata": {},
   "source": [
    "Select MULTIPLE NEW training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e80891",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_points = 5\n",
    "#-------------------------------------------Selecting MULTIPLE NEW training point\n",
    "# U_f = np.abs(Mean_muY_MC) / Stdv_muY_MC\n",
    "U_f = Stdv_muY_MC\n",
    "\n",
    "U_min_args = np.argsort(U_f.reshape(-1))     #ordering arguments from min to max\n",
    "X_new = Xtest[U_min_args[:learning_points]]  #choosing a given number of MC samples from the minimum U values\n",
    "X = np.concatenate((X, X_new), axis=0)\n",
    "\n",
    "Y_new = function(X_new)\n",
    "Y = np.concatenate((Y, Y_new), axis=0)\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3824806",
   "metadata": {},
   "source": [
    "Select SINGLE NEW Training point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5366154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------Selecting SINGLE NEW training point\n",
    "U_f = np.abs(Mean_muY_MC) / Stdv_muY_MC\n",
    "U_min = np.argmin(U_f)\n",
    "X_new = Xtest[U_min].reshape(-1, dim)\n",
    "# X = np.concatenate((X, X_new), axis=0)\n",
    "\n",
    "Y_new = function(X_new)\n",
    "# Y = np.concatenate((Y, Y_new), axis=0)\n",
    "print(Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
